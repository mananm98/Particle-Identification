{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task is to train a classifier to identify type of a particle. There are six particle types: electron, proton, muon, kaon, pion and ghost. Ghost is a particle with other type than the first five or a detector noise.**\n",
    "\n",
    "### **Different particle types remain different responses in the detector systems or subdetectors. Thre are five systems: tracking system, ring imaging Cherenkov detector (RICH), electromagnetic and hadron calorimeters, and muon system.**\n",
    "\n",
    "\n",
    "![features](https://raw.githubusercontent.com/hse-aml/hadron-collider-machine-learning/64a81c5fd465f68713dace0d400325cd004d39de/week2/pic/pid.jpg)\n",
    "\n",
    "### **Tracking System**\n",
    "***\n",
    "Calculates the path and momentum of charged particles like proton, electron, kaon, pion.\n",
    "\n",
    "### **Ring imaging Cherenkov detector (RICH)**\n",
    "***\n",
    "Identifies particle type, using momentum and path of particle obtained in Tracking System.\n",
    "\n",
    "### **Elctromagnetic Calorimeter**\n",
    "***\n",
    "It can detect electron and photon. It calculates the energy of electron and photon. Electron and photon are consumed in this chamber.\n",
    "\n",
    "### **Hadron Calorimeter**\n",
    "***\n",
    "It calculates energy of all the other particles. All the particles except muon are consumed within this chamber\n",
    "\n",
    "### **Muon System**\n",
    "***\n",
    "muons are detected in this system. It calculates the energy of muons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/mananmehta/Downloads/training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrackP</th>\n",
       "      <th>TrackNDoFSubdetector2</th>\n",
       "      <th>BremDLLbeElectron</th>\n",
       "      <th>MuonLooseFlag</th>\n",
       "      <th>FlagSpd</th>\n",
       "      <th>SpdE</th>\n",
       "      <th>EcalDLLbeElectron</th>\n",
       "      <th>DLLmuon</th>\n",
       "      <th>RICHpFlagElectron</th>\n",
       "      <th>EcalDLLbeMuon</th>\n",
       "      <th>...</th>\n",
       "      <th>TrackNDoF</th>\n",
       "      <th>RICHpFlagMuon</th>\n",
       "      <th>RICH_DLLbeKaon</th>\n",
       "      <th>RICH_DLLbeElectron</th>\n",
       "      <th>HcalE</th>\n",
       "      <th>MuonFlag</th>\n",
       "      <th>FlagMuon</th>\n",
       "      <th>PrsE</th>\n",
       "      <th>RICH_DLLbeMuon</th>\n",
       "      <th>RICH_DLLbeProton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74791.156263</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.232275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-2.505719</td>\n",
       "      <td>6.604153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.929960</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-7.213300</td>\n",
       "      <td>-0.280200</td>\n",
       "      <td>5586.589846</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.422315</td>\n",
       "      <td>-2.081143e-07</td>\n",
       "      <td>-24.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2738.489989</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.357748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.864351</td>\n",
       "      <td>0.263651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.061959</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.324317</td>\n",
       "      <td>1.707283</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.334935</td>\n",
       "      <td>2.771583e+00</td>\n",
       "      <td>-0.648017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2161.409908</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15277.730490</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.638984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-2.533918</td>\n",
       "      <td>-8.724949</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.253981</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-35.202221</td>\n",
       "      <td>-14.742319</td>\n",
       "      <td>4482.803707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.194175</td>\n",
       "      <td>-3.070819e+00</td>\n",
       "      <td>-29.291519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7563.700195</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-0.638962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-2.087146</td>\n",
       "      <td>-7.060422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.995816</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.084287</td>\n",
       "      <td>-10.272412</td>\n",
       "      <td>5107.554680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-5.373712e+00</td>\n",
       "      <td>23.653087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TrackP  TrackNDoFSubdetector2  BremDLLbeElectron  MuonLooseFlag  \\\n",
       "0  74791.156263                   15.0           0.232275            1.0   \n",
       "1   2738.489989                   15.0          -0.357748            0.0   \n",
       "2   2161.409908                   17.0        -999.000000            0.0   \n",
       "3  15277.730490                   20.0          -0.638984            0.0   \n",
       "4   7563.700195                   19.0          -0.638962            0.0   \n",
       "\n",
       "   FlagSpd   SpdE  EcalDLLbeElectron     DLLmuon  RICHpFlagElectron  \\\n",
       "0      1.0    3.2          -2.505719    6.604153                1.0   \n",
       "1      1.0    3.2           1.864351    0.263651                1.0   \n",
       "2      0.0 -999.0        -999.000000 -999.000000                0.0   \n",
       "3      1.0    3.2          -2.533918   -8.724949                1.0   \n",
       "4      1.0    3.2          -2.087146   -7.060422                1.0   \n",
       "\n",
       "   EcalDLLbeMuon        ...         TrackNDoF  RICHpFlagMuon  RICH_DLLbeKaon  \\\n",
       "0       1.929960        ...              28.0            1.0       -7.213300   \n",
       "1      -2.061959        ...              32.0            1.0       -0.324317   \n",
       "2    -999.000000        ...              27.0            0.0     -999.000000   \n",
       "3      -3.253981        ...              36.0            1.0      -35.202221   \n",
       "4      -0.995816        ...              33.0            1.0       25.084287   \n",
       "\n",
       "   RICH_DLLbeElectron        HcalE  MuonFlag  FlagMuon        PrsE  \\\n",
       "0           -0.280200  5586.589846       1.0       1.0   10.422315   \n",
       "1            1.707283    -0.000007       0.0       1.0   43.334935   \n",
       "2         -999.000000  -999.000000       0.0       0.0 -999.000000   \n",
       "3          -14.742319  4482.803707       0.0       1.0    2.194175   \n",
       "4          -10.272412  5107.554680       0.0       1.0    0.000015   \n",
       "\n",
       "   RICH_DLLbeMuon  RICH_DLLbeProton  \n",
       "0   -2.081143e-07        -24.824400  \n",
       "1    2.771583e+00         -0.648017  \n",
       "2   -9.990000e+02       -999.000000  \n",
       "3   -3.070819e+00        -29.291519  \n",
       "4   -5.373712e+00         23.653087  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have 50 features each having 12,00,000 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200000 entries, 0 to 1199999\n",
      "Data columns (total 50 columns):\n",
      "TrackP                             1200000 non-null float64\n",
      "TrackNDoFSubdetector2              1200000 non-null float64\n",
      "BremDLLbeElectron                  1200000 non-null float64\n",
      "MuonLooseFlag                      1200000 non-null float64\n",
      "FlagSpd                            1200000 non-null float64\n",
      "SpdE                               1200000 non-null float64\n",
      "EcalDLLbeElectron                  1200000 non-null float64\n",
      "DLLmuon                            1200000 non-null float64\n",
      "RICHpFlagElectron                  1200000 non-null float64\n",
      "EcalDLLbeMuon                      1200000 non-null float64\n",
      "TrackQualitySubdetector2           1200000 non-null float64\n",
      "FlagPrs                            1200000 non-null float64\n",
      "DLLelectron                        1200000 non-null float64\n",
      "DLLkaon                            1200000 non-null float64\n",
      "EcalE                              1200000 non-null float64\n",
      "TrackQualityPerNDoF                1200000 non-null float64\n",
      "DLLproton                          1200000 non-null float64\n",
      "PrsDLLbeElectron                   1200000 non-null float64\n",
      "FlagRICH1                          1200000 non-null float64\n",
      "MuonLLbeBCK                        1200000 non-null float64\n",
      "FlagHcal                           1200000 non-null float64\n",
      "EcalShowerLongitudinalParameter    1200000 non-null float64\n",
      "Calo2dFitQuality                   1200000 non-null float64\n",
      "TrackPt                            1200000 non-null float64\n",
      "TrackDistanceToZ                   1200000 non-null float64\n",
      "RICHpFlagPion                      1200000 non-null float64\n",
      "HcalDLLbeElectron                  1200000 non-null float64\n",
      "Calo3dFitQuality                   1200000 non-null float64\n",
      "FlagEcal                           1200000 non-null float64\n",
      "MuonLLbeMuon                       1200000 non-null float64\n",
      "TrackNDoFSubdetector1              1200000 non-null float64\n",
      "RICHpFlagProton                    1200000 non-null float64\n",
      "RICHpFlagKaon                      1200000 non-null float64\n",
      "GhostProbability                   1200000 non-null float64\n",
      "TrackQualitySubdetector1           1200000 non-null float64\n",
      "Label                              1200000 non-null object\n",
      "RICH_DLLbeBCK                      1200000 non-null float64\n",
      "FlagRICH2                          1200000 non-null float64\n",
      "FlagBrem                           1200000 non-null float64\n",
      "HcalDLLbeMuon                      1200000 non-null float64\n",
      "TrackNDoF                          1200000 non-null float64\n",
      "RICHpFlagMuon                      1200000 non-null float64\n",
      "RICH_DLLbeKaon                     1200000 non-null float64\n",
      "RICH_DLLbeElectron                 1200000 non-null float64\n",
      "HcalE                              1200000 non-null float64\n",
      "MuonFlag                           1200000 non-null float64\n",
      "FlagMuon                           1200000 non-null float64\n",
      "PrsE                               1200000 non-null float64\n",
      "RICH_DLLbeMuon                     1200000 non-null float64\n",
      "RICH_DLLbeProton                   1200000 non-null float64\n",
      "dtypes: float64(49), object(1)\n",
      "memory usage: 457.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrackP</th>\n",
       "      <th>TrackNDoFSubdetector2</th>\n",
       "      <th>BremDLLbeElectron</th>\n",
       "      <th>MuonLooseFlag</th>\n",
       "      <th>FlagSpd</th>\n",
       "      <th>SpdE</th>\n",
       "      <th>EcalDLLbeElectron</th>\n",
       "      <th>DLLmuon</th>\n",
       "      <th>RICHpFlagElectron</th>\n",
       "      <th>EcalDLLbeMuon</th>\n",
       "      <th>...</th>\n",
       "      <th>TrackNDoF</th>\n",
       "      <th>RICHpFlagMuon</th>\n",
       "      <th>RICH_DLLbeKaon</th>\n",
       "      <th>RICH_DLLbeElectron</th>\n",
       "      <th>HcalE</th>\n",
       "      <th>MuonFlag</th>\n",
       "      <th>FlagMuon</th>\n",
       "      <th>PrsE</th>\n",
       "      <th>RICH_DLLbeMuon</th>\n",
       "      <th>RICH_DLLbeProton</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "      <td>1.200000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.614162e+04</td>\n",
       "      <td>1.475170e+01</td>\n",
       "      <td>-2.077315e+02</td>\n",
       "      <td>1.908400e-01</td>\n",
       "      <td>8.531908e-01</td>\n",
       "      <td>-1.443849e+02</td>\n",
       "      <td>-1.828275e+02</td>\n",
       "      <td>-1.266902e+01</td>\n",
       "      <td>9.514050e-01</td>\n",
       "      <td>-1.830022e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.942221e+01</td>\n",
       "      <td>9.187425e-01</td>\n",
       "      <td>-5.183805e+01</td>\n",
       "      <td>-5.103860e+01</td>\n",
       "      <td>2.900030e+03</td>\n",
       "      <td>1.662333e-01</td>\n",
       "      <td>8.209933e-01</td>\n",
       "      <td>-1.338948e+02</td>\n",
       "      <td>-4.919965e+01</td>\n",
       "      <td>-5.223401e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.790366e+04</td>\n",
       "      <td>4.038073e+00</td>\n",
       "      <td>4.054632e+02</td>\n",
       "      <td>3.929634e-01</td>\n",
       "      <td>3.539157e-01</td>\n",
       "      <td>3.545035e+02</td>\n",
       "      <td>3.854803e+02</td>\n",
       "      <td>1.049158e+02</td>\n",
       "      <td>2.150199e-01</td>\n",
       "      <td>3.853958e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>6.029159e+00</td>\n",
       "      <td>2.732303e-01</td>\n",
       "      <td>2.153209e+02</td>\n",
       "      <td>2.150846e+02</td>\n",
       "      <td>7.686446e+03</td>\n",
       "      <td>3.722901e-01</td>\n",
       "      <td>3.833581e-01</td>\n",
       "      <td>3.628977e+02</td>\n",
       "      <td>2.147605e+02</td>\n",
       "      <td>2.152330e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.115380e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "      <td>-9.990000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.137650e+03</td>\n",
       "      <td>1.300000e+01</td>\n",
       "      <td>-6.389485e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-3.000639e+00</td>\n",
       "      <td>-5.711142e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-3.367626e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.469941e+01</td>\n",
       "      <td>-1.310509e+01</td>\n",
       "      <td>-1.438680e-05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.222748e-06</td>\n",
       "      <td>-4.503331e+00</td>\n",
       "      <td>-1.619484e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.069835e+03</td>\n",
       "      <td>1.600000e+01</td>\n",
       "      <td>-5.329026e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.200000e+00</td>\n",
       "      <td>-2.321410e+00</td>\n",
       "      <td>-1.988139e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.956064e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-1.207258e-06</td>\n",
       "      <td>-2.356098e+00</td>\n",
       "      <td>5.780127e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.468449e+00</td>\n",
       "      <td>-4.697510e-01</td>\n",
       "      <td>-2.928758e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.750157e+04</td>\n",
       "      <td>1.800000e+01</td>\n",
       "      <td>-5.933819e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.200000e+00</td>\n",
       "      <td>5.918621e-01</td>\n",
       "      <td>1.669210e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.339380e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.465107e+00</td>\n",
       "      <td>1.674137e+00</td>\n",
       "      <td>3.046122e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>8.502449e+00</td>\n",
       "      <td>1.300086e+00</td>\n",
       "      <td>8.922095e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.750951e+06</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>4.791513e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.200000e+00</td>\n",
       "      <td>4.341298e+00</td>\n",
       "      <td>1.471079e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.153017e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>5.200000e+01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.588015e+02</td>\n",
       "      <td>1.861542e+02</td>\n",
       "      <td>8.682200e+05</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.805800e+02</td>\n",
       "      <td>1.428335e+02</td>\n",
       "      <td>1.462984e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             TrackP  TrackNDoFSubdetector2  BremDLLbeElectron  MuonLooseFlag  \\\n",
       "count  1.200000e+06           1.200000e+06       1.200000e+06   1.200000e+06   \n",
       "mean   1.614162e+04           1.475170e+01      -2.077315e+02   1.908400e-01   \n",
       "std    2.790366e+04           4.038073e+00       4.054632e+02   3.929634e-01   \n",
       "min    1.115380e+03           1.000000e+00      -9.990000e+02   0.000000e+00   \n",
       "25%    4.137650e+03           1.300000e+01      -6.389485e-01   0.000000e+00   \n",
       "50%    8.069835e+03           1.600000e+01      -5.329026e-01   0.000000e+00   \n",
       "75%    1.750157e+04           1.800000e+01      -5.933819e-02   0.000000e+00   \n",
       "max    4.750951e+06           3.000000e+01       4.791513e+00   1.000000e+00   \n",
       "\n",
       "            FlagSpd          SpdE  EcalDLLbeElectron       DLLmuon  \\\n",
       "count  1.200000e+06  1.200000e+06       1.200000e+06  1.200000e+06   \n",
       "mean   8.531908e-01 -1.443849e+02      -1.828275e+02 -1.266902e+01   \n",
       "std    3.539157e-01  3.545035e+02       3.854803e+02  1.049158e+02   \n",
       "min    0.000000e+00 -9.990000e+02      -9.990000e+02 -9.990000e+02   \n",
       "25%    1.000000e+00  0.000000e+00      -3.000639e+00 -5.711142e+00   \n",
       "50%    1.000000e+00  3.200000e+00      -2.321410e+00 -1.988139e+00   \n",
       "75%    1.000000e+00  3.200000e+00       5.918621e-01  1.669210e+00   \n",
       "max    1.000000e+00  3.200000e+00       4.341298e+00  1.471079e+01   \n",
       "\n",
       "       RICHpFlagElectron  EcalDLLbeMuon        ...            TrackNDoF  \\\n",
       "count       1.200000e+06   1.200000e+06        ...         1.200000e+06   \n",
       "mean        9.514050e-01  -1.830022e+02        ...         2.942221e+01   \n",
       "std         2.150199e-01   3.853958e+02        ...         6.029159e+00   \n",
       "min         0.000000e+00  -9.990000e+02        ...         7.000000e+00   \n",
       "25%         1.000000e+00  -3.367626e+00        ...         2.600000e+01   \n",
       "50%         1.000000e+00  -1.956064e+00        ...         3.000000e+01   \n",
       "75%         1.000000e+00   4.339380e-01        ...         3.400000e+01   \n",
       "max         1.000000e+00   2.153017e+00        ...         5.200000e+01   \n",
       "\n",
       "       RICHpFlagMuon  RICH_DLLbeKaon  RICH_DLLbeElectron         HcalE  \\\n",
       "count   1.200000e+06    1.200000e+06        1.200000e+06  1.200000e+06   \n",
       "mean    9.187425e-01   -5.183805e+01       -5.103860e+01  2.900030e+03   \n",
       "std     2.732303e-01    2.153209e+02        2.150846e+02  7.686446e+03   \n",
       "min     0.000000e+00   -9.990000e+02       -9.990000e+02 -9.990000e+02   \n",
       "25%     1.000000e+00   -1.469941e+01       -1.310509e+01 -1.438680e-05   \n",
       "50%     1.000000e+00   -1.207258e-06       -2.356098e+00  5.780127e+02   \n",
       "75%     1.000000e+00    8.465107e+00        1.674137e+00  3.046122e+03   \n",
       "max     1.000000e+00    1.588015e+02        1.861542e+02  8.682200e+05   \n",
       "\n",
       "           MuonFlag      FlagMuon          PrsE  RICH_DLLbeMuon  \\\n",
       "count  1.200000e+06  1.200000e+06  1.200000e+06    1.200000e+06   \n",
       "mean   1.662333e-01  8.209933e-01 -1.338948e+02   -4.919965e+01   \n",
       "std    3.722901e-01  3.833581e-01  3.628977e+02    2.147605e+02   \n",
       "min    0.000000e+00  0.000000e+00 -9.990000e+02   -9.990000e+02   \n",
       "25%    0.000000e+00  1.000000e+00  9.222748e-06   -4.503331e+00   \n",
       "50%    0.000000e+00  1.000000e+00  2.468449e+00   -4.697510e-01   \n",
       "75%    0.000000e+00  1.000000e+00  8.502449e+00    1.300086e+00   \n",
       "max    1.000000e+00  1.000000e+00  2.805800e+02    1.428335e+02   \n",
       "\n",
       "       RICH_DLLbeProton  \n",
       "count      1.200000e+06  \n",
       "mean      -5.223401e+01  \n",
       "std        2.152330e+02  \n",
       "min       -9.990000e+02  \n",
       "25%       -1.619484e+01  \n",
       "50%       -2.928758e-06  \n",
       "75%        8.922095e+00  \n",
       "max        1.462984e+02  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of features\n",
    "\n",
    "- ID - id value for tracks (presents only in the test file for the submitting purposes)\n",
    "- Label - string valued observable denoting particle types. Can take values \"Electron\", \"Muon\", \"Kaon\", \"Proton\", \"Pion\" and \"Ghost\". This column is absent in the test file.\n",
    "- FlagSpd - flag (0 or 1), if reconstructed track passes through Spd\n",
    "- FlagPrs - flag (0 or 1), if reconstructed track passes through Prs\n",
    "- FlagBrem - flag (0 or 1), if reconstructed track passes through Brem\n",
    "- FlagEcal - flag (0 or 1), if reconstructed track passes through Ecal\n",
    "- FlagHcal - flag (0 or 1), if reconstructed track passes through Hcal\n",
    "- FlagRICH1 - flag (0 or 1), if reconstructed track passes through the first RICH detector\n",
    "- FlagRICH2 - flag (0 or 1), if reconstructed track passes through the second RICH detector\n",
    "- FlagMuon - flag (0 or 1), if reconstructed track passes through muon stations (Muon)\n",
    "- SpdE - energy deposit associated to the track in the Spd\n",
    "- PrsE - energy deposit associated to the track in the Prs\n",
    "- EcalE - energy deposit associated to the track in the Hcal\n",
    "- HcalE - energy deposit associated to the track in the Hcal\n",
    "- PrsDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Prs\n",
    "- BremDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Brem\n",
    "- TrackP - particle momentum\n",
    "- TrackPt - particle transverse momentum\n",
    "- TrackNDoFSubdetector1  - number of degrees of freedom for track fit using hits in the tracking sub-detector1\n",
    "- TrackQualitySubdetector1 - chi2 quality of the track fit using hits in the tracking sub-detector1\n",
    "- TrackNDoFSubdetector2 - number of degrees of freedom for track fit using hits in the tracking sub-detector2\n",
    "- TrackQualitySubdetector2 - chi2 quality of the track fit using hits in the  tracking sub-detector2\n",
    "- TrackNDoF - number of degrees of freedom for track fit using hits in all tracking sub-detectors\n",
    "- TrackQualityPerNDoF - chi2 quality of the track fit per degree of freedom\n",
    "- TrackDistanceToZ - distance between track and z-axis (beam axis)\n",
    "- Calo2dFitQuality - quality of the 2d fit of the clusters in the calorimeter \n",
    "- Calo3dFitQuality - quality of the 3d fit in the calorimeter with assumption that particle was electron\n",
    "- EcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Ecal\n",
    "- EcalDLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from Ecal\n",
    "- EcalShowerLongitudinalParameter - longitudinal parameter of Ecal shower\n",
    "- HcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Hcal\n",
    "- HcalDLLbeMuon - delta log-likelihood for a particle candidate to be using information from Hcal\n",
    "- RICHpFlagElectron - flag (0 or 1) if momentum is greater than threshold for electrons to produce Cherenkov light\n",
    "- RICHpFlagProton - flag (0 or 1) if momentum is greater than threshold for protons to produce Cherenkov light\n",
    "- RICHpFlagPion - flag (0 or 1) if momentum is greater than threshold for pions to produce Cherenkov light\n",
    "- RICHpFlagKaon - flag (0 or 1) if momentum is greater than threshold for kaons to produce Cherenkov light\n",
    "- RICHpFlagMuon - flag (0 or 1) if momentum is greater than threshold for muons to produce Cherenkov light\n",
    "- RICH_DLLbeBCK  - delta log-likelihood for a particle candidate to be background using information from RICH\n",
    "- RICH_DLLbeKaon - delta log-likelihood for a particle candidate to be kaon using information from RICH\n",
    "- RICH_DLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from RICH\n",
    "- RICH_DLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from RICH\n",
    "- RICH_DLLbeProton - delta log-likelihood for a particle candidate to be proton using information from RICH\n",
    "- MuonFlag - muon flag (is this track muon) which is determined from muon stations\n",
    "- MuonLooseFlag muon flag (is this track muon) which is determined from muon stations using looser criteria\n",
    "- MuonLLbeBCK - log-likelihood for a particle candidate to be not muon using information from muon stations\n",
    "- MuonLLbeMuon - log-likelihood for a particle candidate to be muon using information from muon stations\n",
    "- DLLelectron - delta log-likelihood for a particle candidate to be electron using information from all subdetectors\n",
    "- DLLmuon - delta log-likelihood for a particle candidate to be muon using information from all subdetectors\n",
    "- DLLkaon - delta log-likelihood for a particle candidate to be kaon using information from all subdetectors\n",
    "- DLLproton - delta log-likelihood for a particle candidate to be proton using information from all subdetectors\n",
    "- GhostProbability - probability for a particle candidate to be ghost track. This variable is an output of classification model used in the tracking algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training features\n",
    "\n",
    "The following set of features describe particle responses in the detector systems:\n",
    "\n",
    "![features](https://raw.githubusercontent.com/hse-aml/hadron-collider-machine-learning/master/week2/pic/features.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have 6 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Muon' 'Ghost' 'Pion' 'Proton' 'Kaon' 'Electron']\n"
     ]
    }
   ],
   "source": [
    "print(df.Label.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverting Labels to one_hot labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh = OneHotEncoder()\n",
    "y = df['Label'].values\n",
    "y = np.reshape(y,(-1,1))\n",
    "y = oh.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separating features data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[list(set(df.columns) - {'Label','class'})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "data_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mananmehta/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(data_scaled,y, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mananmehta/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960000 960000\n",
      "240000 240000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train),len(y_train))\n",
    "print(len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network to classify Particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mananmehta/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                1600      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 1,798\n",
      "Trainable params: 1,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#opt = Adam(lr = 0.003)\n",
    "model = Sequential()\n",
    "model.add(Dense(32,activation = 'relu',input_shape = (49,)))\n",
    "#model.add(Dense(8,activation = 'relu'))\n",
    "\n",
    "model.add((Dense(6,activation = 'softmax')))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 768000 samples, validate on 192000 samples\n",
      "Epoch 1/500\n",
      "768000/768000 [==============================] - 7s 10us/step - loss: 5.6622 - acc: 0.5045 - val_loss: 5.4288 - val_acc: 0.5294\n",
      "Epoch 2/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 5.3269 - acc: 0.5385 - val_loss: 5.2707 - val_acc: 0.5393\n",
      "Epoch 3/500\n",
      "768000/768000 [==============================] - 4s 5us/step - loss: 5.2464 - acc: 0.5462 - val_loss: 5.2451 - val_acc: 0.5399\n",
      "Epoch 4/500\n",
      "768000/768000 [==============================] - 4s 5us/step - loss: 5.2421 - acc: 0.5481 - val_loss: 5.2194 - val_acc: 0.5504\n",
      "Epoch 5/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 5.2380 - acc: 0.5492 - val_loss: 5.2165 - val_acc: 0.5517\n",
      "Epoch 6/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 5.1557 - acc: 0.5553 - val_loss: 5.0086 - val_acc: 0.5598\n",
      "Epoch 7/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 5.0132 - acc: 0.5640 - val_loss: 4.9953 - val_acc: 0.5662\n",
      "Epoch 8/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 5.0081 - acc: 0.5648 - val_loss: 5.0052 - val_acc: 0.5647\n",
      "Epoch 9/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 5.0053 - acc: 0.5658 - val_loss: 4.9928 - val_acc: 0.5651\n",
      "Epoch 10/500\n",
      "768000/768000 [==============================] - 4s 5us/step - loss: 4.9999 - acc: 0.5661 - val_loss: 5.0055 - val_acc: 0.5672\n",
      "Epoch 11/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.9894 - acc: 0.5664 - val_loss: 4.9820 - val_acc: 0.5686\n",
      "Epoch 12/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.9839 - acc: 0.5679 - val_loss: 4.9772 - val_acc: 0.5698\n",
      "Epoch 13/500\n",
      "768000/768000 [==============================] - 3s 5us/step - loss: 4.9824 - acc: 0.5683 - val_loss: 4.9912 - val_acc: 0.5608\n",
      "Epoch 14/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.6665 - acc: 0.5852 - val_loss: 4.5246 - val_acc: 0.5905\n",
      "Epoch 15/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.5346 - acc: 0.5927 - val_loss: 4.5137 - val_acc: 0.5966\n",
      "Epoch 16/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.5322 - acc: 0.5931 - val_loss: 4.5332 - val_acc: 0.5900\n",
      "Epoch 17/500\n",
      "768000/768000 [==============================] - 3s 5us/step - loss: 4.5304 - acc: 0.5937 - val_loss: 4.5298 - val_acc: 0.5910\n",
      "Epoch 18/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.5296 - acc: 0.5936 - val_loss: 4.5146 - val_acc: 0.5958\n",
      "Epoch 19/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.5270 - acc: 0.5944 - val_loss: 4.5126 - val_acc: 0.5966\n",
      "Epoch 20/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4594 - acc: 0.5974 - val_loss: 4.4188 - val_acc: 0.6006\n",
      "Epoch 21/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4577 - acc: 0.5972 - val_loss: 4.5222 - val_acc: 0.5950\n",
      "Epoch 22/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.5272 - acc: 0.5933 - val_loss: 4.5333 - val_acc: 0.5896\n",
      "Epoch 23/500\n",
      "768000/768000 [==============================] - 3s 5us/step - loss: 4.5101 - acc: 0.5942 - val_loss: 4.4214 - val_acc: 0.5993\n",
      "Epoch 24/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4314 - acc: 0.5997 - val_loss: 4.4231 - val_acc: 0.5976\n",
      "Epoch 25/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4315 - acc: 0.6000 - val_loss: 4.4170 - val_acc: 0.6017\n",
      "Epoch 26/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4301 - acc: 0.6001 - val_loss: 4.4151 - val_acc: 0.6009\n",
      "Epoch 27/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4294 - acc: 0.6004 - val_loss: 4.4161 - val_acc: 0.6015\n",
      "Epoch 28/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4294 - acc: 0.6002 - val_loss: 4.4172 - val_acc: 0.6009\n",
      "Epoch 29/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.4273 - acc: 0.6014 - val_loss: 4.4178 - val_acc: 0.6013\n",
      "Epoch 30/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.3896 - acc: 0.6022 - val_loss: 4.3088 - val_acc: 0.6011\n",
      "Epoch 31/500\n",
      "768000/768000 [==============================] - 4s 5us/step - loss: 4.3140 - acc: 0.6051 - val_loss: 4.3036 - val_acc: 0.6057\n",
      "Epoch 32/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.3137 - acc: 0.6054 - val_loss: 4.3020 - val_acc: 0.6051\n",
      "Epoch 33/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.3130 - acc: 0.6054 - val_loss: 4.3001 - val_acc: 0.6066\n",
      "Epoch 34/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.3091 - acc: 0.6061 - val_loss: 4.1361 - val_acc: 0.6063\n",
      "Epoch 35/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.0825 - acc: 0.6147 - val_loss: 4.0602 - val_acc: 0.6169\n",
      "Epoch 36/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.0689 - acc: 0.6162 - val_loss: 4.0479 - val_acc: 0.6161\n",
      "Epoch 37/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.0643 - acc: 0.6162 - val_loss: 4.0623 - val_acc: 0.6141\n",
      "Epoch 38/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 4.0607 - acc: 0.6166 - val_loss: 4.0407 - val_acc: 0.6159\n",
      "Epoch 39/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.9592 - acc: 0.6187 - val_loss: 3.9218 - val_acc: 0.6143\n",
      "Epoch 40/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.9336 - acc: 0.6196 - val_loss: 3.9230 - val_acc: 0.6172\n",
      "Epoch 41/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.9280 - acc: 0.6208 - val_loss: 3.9077 - val_acc: 0.6212\n",
      "Epoch 42/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.7715 - acc: 0.6247 - val_loss: 3.6953 - val_acc: 0.6204\n",
      "Epoch 43/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.7130 - acc: 0.6267 - val_loss: 3.7058 - val_acc: 0.6248\n",
      "Epoch 44/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.7118 - acc: 0.6273 - val_loss: 3.6968 - val_acc: 0.6262\n",
      "Epoch 45/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.7019 - acc: 0.6275 - val_loss: 3.6611 - val_acc: 0.6279\n",
      "Epoch 46/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.6451 - acc: 0.6287 - val_loss: 3.6290 - val_acc: 0.6273\n",
      "Epoch 47/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.6307 - acc: 0.6295 - val_loss: 3.6114 - val_acc: 0.6298\n",
      "Epoch 48/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.6042 - acc: 0.6309 - val_loss: 3.5656 - val_acc: 0.6315\n",
      "Epoch 49/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.5837 - acc: 0.6321 - val_loss: 3.5636 - val_acc: 0.6312\n",
      "Epoch 50/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 3.5818 - acc: 0.6316 - val_loss: 3.5588 - val_acc: 0.6315\n",
      "Epoch 51/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.9926 - acc: 0.6503 - val_loss: 2.5838 - val_acc: 0.6515\n",
      "Epoch 52/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.4619 - acc: 0.6623 - val_loss: 2.3534 - val_acc: 0.6696\n",
      "Epoch 53/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.3229 - acc: 0.6717 - val_loss: 2.2593 - val_acc: 0.6783\n",
      "Epoch 54/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2703 - acc: 0.6769 - val_loss: 2.2455 - val_acc: 0.6822\n",
      "Epoch 55/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2609 - acc: 0.6797 - val_loss: 2.2707 - val_acc: 0.6751\n",
      "Epoch 56/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2550 - acc: 0.6805 - val_loss: 2.2421 - val_acc: 0.6805\n",
      "Epoch 57/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2487 - acc: 0.6819 - val_loss: 2.2428 - val_acc: 0.6806\n",
      "Epoch 58/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2464 - acc: 0.6825 - val_loss: 2.2394 - val_acc: 0.6813\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2460 - acc: 0.6826 - val_loss: 2.2360 - val_acc: 0.6825\n",
      "Epoch 60/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2421 - acc: 0.6835 - val_loss: 2.2298 - val_acc: 0.6833\n",
      "Epoch 61/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2422 - acc: 0.6835 - val_loss: 2.2259 - val_acc: 0.6856\n",
      "Epoch 62/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2395 - acc: 0.6846 - val_loss: 2.2316 - val_acc: 0.6822\n",
      "Epoch 63/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2384 - acc: 0.6849 - val_loss: 2.2284 - val_acc: 0.6848\n",
      "Epoch 64/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2378 - acc: 0.6849 - val_loss: 2.2312 - val_acc: 0.6828\n",
      "Epoch 65/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2360 - acc: 0.6844 - val_loss: 2.2059 - val_acc: 0.6859\n",
      "Epoch 66/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2142 - acc: 0.6853 - val_loss: 2.2063 - val_acc: 0.6851\n",
      "Epoch 67/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2137 - acc: 0.6856 - val_loss: 2.2075 - val_acc: 0.6837\n",
      "Epoch 68/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2124 - acc: 0.6857 - val_loss: 2.2024 - val_acc: 0.6853\n",
      "Epoch 69/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2126 - acc: 0.6858 - val_loss: 2.2017 - val_acc: 0.6856\n",
      "Epoch 70/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 2.2114 - acc: 0.6860 - val_loss: 2.2003 - val_acc: 0.6872\n",
      "Epoch 71/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 1.9759 - acc: 0.6893 - val_loss: 1.8740 - val_acc: 0.6884\n",
      "Epoch 72/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 1.2415 - acc: 0.7093 - val_loss: 0.9049 - val_acc: 0.7187\n",
      "Epoch 73/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.8609 - acc: 0.7212 - val_loss: 0.8573 - val_acc: 0.7209\n",
      "Epoch 74/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.8453 - acc: 0.7240 - val_loss: 0.8469 - val_acc: 0.7202\n",
      "Epoch 75/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.8421 - acc: 0.7248 - val_loss: 0.8398 - val_acc: 0.7251\n",
      "Epoch 76/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.8400 - acc: 0.7259 - val_loss: 0.8424 - val_acc: 0.7243\n",
      "Epoch 77/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.8118 - acc: 0.7261 - val_loss: 0.7120 - val_acc: 0.7209\n",
      "Epoch 78/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.7037 - acc: 0.7272 - val_loss: 0.7021 - val_acc: 0.7277\n",
      "Epoch 79/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6994 - acc: 0.7293 - val_loss: 0.6993 - val_acc: 0.7295\n",
      "Epoch 80/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6939 - acc: 0.7321 - val_loss: 0.6946 - val_acc: 0.7321\n",
      "Epoch 81/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6907 - acc: 0.7333 - val_loss: 0.6914 - val_acc: 0.7344\n",
      "Epoch 82/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6879 - acc: 0.7346 - val_loss: 0.6856 - val_acc: 0.7367\n",
      "Epoch 83/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6851 - acc: 0.7358 - val_loss: 0.6823 - val_acc: 0.7358\n",
      "Epoch 84/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6832 - acc: 0.7365 - val_loss: 0.6854 - val_acc: 0.7347\n",
      "Epoch 85/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6814 - acc: 0.7376 - val_loss: 0.6872 - val_acc: 0.7346\n",
      "Epoch 86/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6797 - acc: 0.7387 - val_loss: 0.6842 - val_acc: 0.7370\n",
      "Epoch 87/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6787 - acc: 0.7391 - val_loss: 0.6775 - val_acc: 0.7396\n",
      "Epoch 88/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6778 - acc: 0.7393 - val_loss: 0.6863 - val_acc: 0.7357\n",
      "Epoch 89/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6763 - acc: 0.7404 - val_loss: 0.6784 - val_acc: 0.7392\n",
      "Epoch 90/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6765 - acc: 0.7398 - val_loss: 0.6755 - val_acc: 0.7409\n",
      "Epoch 91/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6752 - acc: 0.7403 - val_loss: 0.6711 - val_acc: 0.7412\n",
      "Epoch 92/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6753 - acc: 0.7400 - val_loss: 0.6795 - val_acc: 0.7386\n",
      "Epoch 93/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6741 - acc: 0.7407 - val_loss: 0.6778 - val_acc: 0.7392\n",
      "Epoch 94/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6734 - acc: 0.7409 - val_loss: 0.6682 - val_acc: 0.7433\n",
      "Epoch 95/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6731 - acc: 0.7407 - val_loss: 0.6822 - val_acc: 0.7358\n",
      "Epoch 96/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6725 - acc: 0.7408 - val_loss: 0.6740 - val_acc: 0.7409\n",
      "Epoch 97/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6722 - acc: 0.7412 - val_loss: 0.6765 - val_acc: 0.7397\n",
      "Epoch 98/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6719 - acc: 0.7409 - val_loss: 0.6692 - val_acc: 0.7423\n",
      "Epoch 99/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6715 - acc: 0.7415 - val_loss: 0.6744 - val_acc: 0.7401\n",
      "Epoch 100/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6708 - acc: 0.7419 - val_loss: 0.6823 - val_acc: 0.7351\n",
      "Epoch 101/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6705 - acc: 0.7415 - val_loss: 0.6715 - val_acc: 0.7413\n",
      "Epoch 102/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6707 - acc: 0.7415 - val_loss: 0.6693 - val_acc: 0.7430\n",
      "Epoch 103/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6702 - acc: 0.7420 - val_loss: 0.6721 - val_acc: 0.7374\n",
      "Epoch 104/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6697 - acc: 0.7416 - val_loss: 0.6691 - val_acc: 0.7434\n",
      "Epoch 105/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6695 - acc: 0.7419 - val_loss: 0.6719 - val_acc: 0.7418\n",
      "Epoch 106/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6690 - acc: 0.7424 - val_loss: 0.6653 - val_acc: 0.7447\n",
      "Epoch 107/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6686 - acc: 0.7422 - val_loss: 0.6675 - val_acc: 0.7431\n",
      "Epoch 108/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6687 - acc: 0.7424 - val_loss: 0.6679 - val_acc: 0.7407\n",
      "Epoch 109/500\n",
      "768000/768000 [==============================] - 3s 5us/step - loss: 0.6680 - acc: 0.7422 - val_loss: 0.6702 - val_acc: 0.7420\n",
      "Epoch 110/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6681 - acc: 0.7426 - val_loss: 0.6689 - val_acc: 0.7401\n",
      "Epoch 111/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6671 - acc: 0.7428 - val_loss: 0.6665 - val_acc: 0.7426\n",
      "Epoch 112/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6674 - acc: 0.7427 - val_loss: 0.6668 - val_acc: 0.7437\n",
      "Epoch 113/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6673 - acc: 0.7427 - val_loss: 0.6651 - val_acc: 0.7432\n",
      "Epoch 114/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6668 - acc: 0.7428 - val_loss: 0.6640 - val_acc: 0.7437\n",
      "Epoch 115/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6660 - acc: 0.7433 - val_loss: 0.6647 - val_acc: 0.7439\n",
      "Epoch 116/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6659 - acc: 0.7429 - val_loss: 0.6623 - val_acc: 0.7444\n",
      "Epoch 117/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6653 - acc: 0.7431 - val_loss: 0.6635 - val_acc: 0.7425\n",
      "Epoch 118/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6661 - acc: 0.7428 - val_loss: 0.6634 - val_acc: 0.7440\n",
      "Epoch 119/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6653 - acc: 0.7434 - val_loss: 0.6655 - val_acc: 0.7415\n",
      "Epoch 120/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6653 - acc: 0.7432 - val_loss: 0.6708 - val_acc: 0.7416\n",
      "Epoch 121/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6649 - acc: 0.7432 - val_loss: 0.6670 - val_acc: 0.7423\n",
      "Epoch 122/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6650 - acc: 0.7431 - val_loss: 0.6676 - val_acc: 0.7423\n",
      "Epoch 123/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6646 - acc: 0.7434 - val_loss: 0.6714 - val_acc: 0.7399\n",
      "Epoch 124/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6639 - acc: 0.7435 - val_loss: 0.6631 - val_acc: 0.7428\n",
      "Epoch 125/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6637 - acc: 0.7437 - val_loss: 0.6659 - val_acc: 0.7419\n",
      "Epoch 126/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6636 - acc: 0.7439 - val_loss: 0.6687 - val_acc: 0.7416\n",
      "Epoch 127/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6636 - acc: 0.7433 - val_loss: 0.6663 - val_acc: 0.7419\n",
      "Epoch 128/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6625 - acc: 0.7440 - val_loss: 0.6656 - val_acc: 0.7410\n",
      "Epoch 129/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6621 - acc: 0.7443 - val_loss: 0.6633 - val_acc: 0.7432\n",
      "Epoch 130/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6622 - acc: 0.7439 - val_loss: 0.6622 - val_acc: 0.7445\n",
      "Epoch 131/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6615 - acc: 0.7439 - val_loss: 0.6625 - val_acc: 0.7450\n",
      "Epoch 132/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6615 - acc: 0.7443 - val_loss: 0.6666 - val_acc: 0.7415\n",
      "Epoch 133/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6615 - acc: 0.7440 - val_loss: 0.6593 - val_acc: 0.7458\n",
      "Epoch 134/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6612 - acc: 0.7444 - val_loss: 0.6667 - val_acc: 0.7428\n",
      "Epoch 135/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6610 - acc: 0.7440 - val_loss: 0.6684 - val_acc: 0.7412\n",
      "Epoch 136/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6606 - acc: 0.7445 - val_loss: 0.6575 - val_acc: 0.7463\n",
      "Epoch 137/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6610 - acc: 0.7441 - val_loss: 0.6640 - val_acc: 0.7432\n",
      "Epoch 138/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6605 - acc: 0.7443 - val_loss: 0.6598 - val_acc: 0.7455\n",
      "Epoch 139/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6604 - acc: 0.7443 - val_loss: 0.6597 - val_acc: 0.7450\n",
      "Epoch 140/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6598 - acc: 0.7441 - val_loss: 0.6635 - val_acc: 0.7422\n",
      "Epoch 141/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6595 - acc: 0.7447 - val_loss: 0.6607 - val_acc: 0.7435\n",
      "Epoch 142/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6596 - acc: 0.7445 - val_loss: 0.6635 - val_acc: 0.7418\n",
      "Epoch 143/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6593 - acc: 0.7447 - val_loss: 0.6609 - val_acc: 0.7444\n",
      "Epoch 144/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6597 - acc: 0.7448 - val_loss: 0.6577 - val_acc: 0.7453\n",
      "Epoch 145/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6595 - acc: 0.7448 - val_loss: 0.6599 - val_acc: 0.7438\n",
      "Epoch 146/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6592 - acc: 0.7450 - val_loss: 0.6602 - val_acc: 0.7450\n",
      "Epoch 147/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6584 - acc: 0.7449 - val_loss: 0.6591 - val_acc: 0.7461\n",
      "Epoch 148/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6588 - acc: 0.7448 - val_loss: 0.6585 - val_acc: 0.7445\n",
      "Epoch 149/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6586 - acc: 0.7447 - val_loss: 0.6598 - val_acc: 0.7445\n",
      "Epoch 150/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6584 - acc: 0.7444 - val_loss: 0.6589 - val_acc: 0.7449\n",
      "Epoch 151/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6580 - acc: 0.7447 - val_loss: 0.6627 - val_acc: 0.7445\n",
      "Epoch 152/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6587 - acc: 0.7447 - val_loss: 0.6578 - val_acc: 0.7454\n",
      "Epoch 153/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6578 - acc: 0.7449 - val_loss: 0.6573 - val_acc: 0.7454\n",
      "Epoch 154/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6579 - acc: 0.7443 - val_loss: 0.6564 - val_acc: 0.7462\n",
      "Epoch 155/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6575 - acc: 0.7455 - val_loss: 0.6573 - val_acc: 0.7459\n",
      "Epoch 156/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6573 - acc: 0.7450 - val_loss: 0.6619 - val_acc: 0.7421\n",
      "Epoch 157/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6572 - acc: 0.7455 - val_loss: 0.6580 - val_acc: 0.7452\n",
      "Epoch 158/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6555 - acc: 0.7450 - val_loss: 0.6277 - val_acc: 0.7445\n",
      "Epoch 159/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6238 - acc: 0.7451 - val_loss: 0.6284 - val_acc: 0.7441\n",
      "Epoch 160/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6232 - acc: 0.7451 - val_loss: 0.6215 - val_acc: 0.7467\n",
      "Epoch 161/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6228 - acc: 0.7453 - val_loss: 0.6225 - val_acc: 0.7455\n",
      "Epoch 162/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6227 - acc: 0.7453 - val_loss: 0.6252 - val_acc: 0.7439\n",
      "Epoch 163/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6224 - acc: 0.7457 - val_loss: 0.6231 - val_acc: 0.7449\n",
      "Epoch 164/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6227 - acc: 0.7455 - val_loss: 0.6288 - val_acc: 0.7432\n",
      "Epoch 165/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6231 - acc: 0.7454 - val_loss: 0.6258 - val_acc: 0.7453\n",
      "Epoch 166/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6224 - acc: 0.7460 - val_loss: 0.6230 - val_acc: 0.7452\n",
      "Epoch 167/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6223 - acc: 0.7458 - val_loss: 0.6260 - val_acc: 0.7439\n",
      "Epoch 168/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6224 - acc: 0.7455 - val_loss: 0.6209 - val_acc: 0.7466\n",
      "Epoch 169/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6222 - acc: 0.7456 - val_loss: 0.6206 - val_acc: 0.7463\n",
      "Epoch 170/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6220 - acc: 0.7460 - val_loss: 0.6209 - val_acc: 0.7466\n",
      "Epoch 171/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6217 - acc: 0.7458 - val_loss: 0.6230 - val_acc: 0.7451\n",
      "Epoch 172/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6220 - acc: 0.7456 - val_loss: 0.6225 - val_acc: 0.7456\n",
      "Epoch 173/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6221 - acc: 0.7461 - val_loss: 0.6226 - val_acc: 0.7456\n",
      "Epoch 174/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6216 - acc: 0.7457 - val_loss: 0.6226 - val_acc: 0.7447\n",
      "Epoch 175/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6217 - acc: 0.7460 - val_loss: 0.6237 - val_acc: 0.7449\n",
      "Epoch 176/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6211 - acc: 0.7462 - val_loss: 0.6213 - val_acc: 0.7468\n",
      "Epoch 177/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6216 - acc: 0.7459 - val_loss: 0.6202 - val_acc: 0.7467\n",
      "Epoch 178/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6214 - acc: 0.7458 - val_loss: 0.6190 - val_acc: 0.7466\n",
      "Epoch 179/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6211 - acc: 0.7461 - val_loss: 0.6258 - val_acc: 0.7439\n",
      "Epoch 180/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6216 - acc: 0.7459 - val_loss: 0.6187 - val_acc: 0.7470\n",
      "Epoch 181/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6210 - acc: 0.7462 - val_loss: 0.6224 - val_acc: 0.7458\n",
      "Epoch 182/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6212 - acc: 0.7459 - val_loss: 0.6233 - val_acc: 0.7448\n",
      "Epoch 183/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6206 - acc: 0.7461 - val_loss: 0.6238 - val_acc: 0.7453\n",
      "Epoch 184/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6209 - acc: 0.7461 - val_loss: 0.6277 - val_acc: 0.7430\n",
      "Epoch 185/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6209 - acc: 0.7461 - val_loss: 0.6225 - val_acc: 0.7448\n",
      "Epoch 186/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6205 - acc: 0.7462 - val_loss: 0.6204 - val_acc: 0.7464\n",
      "Epoch 187/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6211 - acc: 0.7460 - val_loss: 0.6205 - val_acc: 0.7462\n",
      "Epoch 188/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6203 - acc: 0.7464 - val_loss: 0.6271 - val_acc: 0.7426\n",
      "Epoch 189/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6204 - acc: 0.7462 - val_loss: 0.6223 - val_acc: 0.7453\n",
      "Epoch 190/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6205 - acc: 0.7459 - val_loss: 0.6182 - val_acc: 0.7470\n",
      "Epoch 191/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6205 - acc: 0.7466 - val_loss: 0.6223 - val_acc: 0.7457\n",
      "Epoch 192/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6198 - acc: 0.7464 - val_loss: 0.6241 - val_acc: 0.7436\n",
      "Epoch 193/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6198 - acc: 0.7463 - val_loss: 0.6217 - val_acc: 0.7452\n",
      "Epoch 194/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6201 - acc: 0.7461 - val_loss: 0.6231 - val_acc: 0.7445\n",
      "Epoch 195/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6206 - acc: 0.7460 - val_loss: 0.6193 - val_acc: 0.7472\n",
      "Epoch 196/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6194 - acc: 0.7464 - val_loss: 0.6188 - val_acc: 0.7468\n",
      "Epoch 197/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6198 - acc: 0.7467 - val_loss: 0.6213 - val_acc: 0.7453\n",
      "Epoch 198/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6201 - acc: 0.7462 - val_loss: 0.6201 - val_acc: 0.7468\n",
      "Epoch 199/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6199 - acc: 0.7462 - val_loss: 0.6167 - val_acc: 0.7477\n",
      "Epoch 200/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6195 - acc: 0.7465 - val_loss: 0.6201 - val_acc: 0.7475\n",
      "Epoch 201/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6194 - acc: 0.7469 - val_loss: 0.6192 - val_acc: 0.7465\n",
      "Epoch 202/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6197 - acc: 0.7463 - val_loss: 0.6184 - val_acc: 0.7474\n",
      "Epoch 203/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6194 - acc: 0.7465 - val_loss: 0.6170 - val_acc: 0.7471\n",
      "Epoch 204/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6196 - acc: 0.7463 - val_loss: 0.6178 - val_acc: 0.7465\n",
      "Epoch 205/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6194 - acc: 0.7465 - val_loss: 0.6213 - val_acc: 0.7465\n",
      "Epoch 206/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6198 - acc: 0.7464 - val_loss: 0.6263 - val_acc: 0.7434\n",
      "Epoch 207/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6199 - acc: 0.7465 - val_loss: 0.6202 - val_acc: 0.7456\n",
      "Epoch 208/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6189 - acc: 0.7465 - val_loss: 0.6194 - val_acc: 0.7456\n",
      "Epoch 209/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6187 - acc: 0.7466 - val_loss: 0.6193 - val_acc: 0.7461\n",
      "Epoch 210/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6191 - acc: 0.7464 - val_loss: 0.6176 - val_acc: 0.7476\n",
      "Epoch 211/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6191 - acc: 0.7465 - val_loss: 0.6199 - val_acc: 0.7469\n",
      "Epoch 212/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6189 - acc: 0.7465 - val_loss: 0.6179 - val_acc: 0.7476\n",
      "Epoch 213/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6189 - acc: 0.7466 - val_loss: 0.6196 - val_acc: 0.7477\n",
      "Epoch 214/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6191 - acc: 0.7465 - val_loss: 0.6196 - val_acc: 0.7463\n",
      "Epoch 215/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6187 - acc: 0.7470 - val_loss: 0.6185 - val_acc: 0.7448\n",
      "Epoch 216/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6190 - acc: 0.7467 - val_loss: 0.6182 - val_acc: 0.7472\n",
      "Epoch 217/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6189 - acc: 0.7465 - val_loss: 0.6188 - val_acc: 0.7461\n",
      "Epoch 218/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6188 - acc: 0.7464 - val_loss: 0.6203 - val_acc: 0.7470\n",
      "Epoch 219/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6188 - acc: 0.7465 - val_loss: 0.6225 - val_acc: 0.7449\n",
      "Epoch 220/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6190 - acc: 0.7468 - val_loss: 0.6173 - val_acc: 0.7467\n",
      "Epoch 221/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6183 - acc: 0.7471 - val_loss: 0.6214 - val_acc: 0.7462\n",
      "Epoch 222/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6189 - acc: 0.7462 - val_loss: 0.6219 - val_acc: 0.7455\n",
      "Epoch 223/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6185 - acc: 0.7467 - val_loss: 0.6190 - val_acc: 0.7465\n",
      "Epoch 224/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6182 - acc: 0.7470 - val_loss: 0.6188 - val_acc: 0.7464\n",
      "Epoch 225/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6187 - acc: 0.7466 - val_loss: 0.6174 - val_acc: 0.7471\n",
      "Epoch 226/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6185 - acc: 0.7470 - val_loss: 0.6228 - val_acc: 0.7441\n",
      "Epoch 227/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6178 - acc: 0.7472 - val_loss: 0.6211 - val_acc: 0.7467\n",
      "Epoch 228/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6180 - acc: 0.7471 - val_loss: 0.6247 - val_acc: 0.7434\n",
      "Epoch 229/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6183 - acc: 0.7468 - val_loss: 0.6186 - val_acc: 0.7453\n",
      "Epoch 230/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6179 - acc: 0.7469 - val_loss: 0.6242 - val_acc: 0.7448\n",
      "Epoch 231/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6185 - acc: 0.7464 - val_loss: 0.6206 - val_acc: 0.7465\n",
      "Epoch 232/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6178 - acc: 0.7467 - val_loss: 0.6171 - val_acc: 0.7480\n",
      "Epoch 233/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6178 - acc: 0.7470 - val_loss: 0.6174 - val_acc: 0.7478\n",
      "Epoch 234/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6180 - acc: 0.7470 - val_loss: 0.6196 - val_acc: 0.7459\n",
      "Epoch 235/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6174 - acc: 0.7471 - val_loss: 0.6187 - val_acc: 0.7454\n",
      "Epoch 236/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6181 - acc: 0.7469 - val_loss: 0.6198 - val_acc: 0.7468\n",
      "Epoch 237/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6173 - acc: 0.7470 - val_loss: 0.6160 - val_acc: 0.7474\n",
      "Epoch 238/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6177 - acc: 0.7468 - val_loss: 0.6174 - val_acc: 0.7462\n",
      "Epoch 239/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6177 - acc: 0.7467 - val_loss: 0.6182 - val_acc: 0.7453\n",
      "Epoch 240/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6176 - acc: 0.7471 - val_loss: 0.6211 - val_acc: 0.7443\n",
      "Epoch 241/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6174 - acc: 0.7475 - val_loss: 0.6204 - val_acc: 0.7459\n",
      "Epoch 242/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6177 - acc: 0.7471 - val_loss: 0.6188 - val_acc: 0.7461\n",
      "Epoch 243/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6176 - acc: 0.7470 - val_loss: 0.6173 - val_acc: 0.7468\n",
      "Epoch 244/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6174 - acc: 0.7471 - val_loss: 0.6270 - val_acc: 0.7418\n",
      "Epoch 245/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6175 - acc: 0.7468 - val_loss: 0.6156 - val_acc: 0.7477\n",
      "Epoch 246/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6173 - acc: 0.7470 - val_loss: 0.6169 - val_acc: 0.7471\n",
      "Epoch 247/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6176 - acc: 0.7472 - val_loss: 0.6189 - val_acc: 0.7455\n",
      "Epoch 248/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6175 - acc: 0.7469 - val_loss: 0.6196 - val_acc: 0.7465\n",
      "Epoch 249/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6173 - acc: 0.7471 - val_loss: 0.6180 - val_acc: 0.7470\n",
      "Epoch 250/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6171 - acc: 0.7471 - val_loss: 0.6190 - val_acc: 0.7467\n",
      "Epoch 251/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6172 - acc: 0.7473 - val_loss: 0.6226 - val_acc: 0.7448\n",
      "Epoch 252/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6170 - acc: 0.7472 - val_loss: 0.6173 - val_acc: 0.7469\n",
      "Epoch 253/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6170 - acc: 0.7473 - val_loss: 0.6179 - val_acc: 0.7460\n",
      "Epoch 254/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6171 - acc: 0.7472 - val_loss: 0.6163 - val_acc: 0.7460\n",
      "Epoch 255/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6172 - acc: 0.7470 - val_loss: 0.6186 - val_acc: 0.7471\n",
      "Epoch 256/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6170 - acc: 0.7473 - val_loss: 0.6210 - val_acc: 0.7474\n",
      "Epoch 257/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6166 - acc: 0.7473 - val_loss: 0.6190 - val_acc: 0.7450\n",
      "Epoch 258/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6170 - acc: 0.7472 - val_loss: 0.6166 - val_acc: 0.7462\n",
      "Epoch 259/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6169 - acc: 0.7470 - val_loss: 0.6168 - val_acc: 0.7476\n",
      "Epoch 260/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6169 - acc: 0.7469 - val_loss: 0.6193 - val_acc: 0.7460\n",
      "Epoch 261/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6162 - acc: 0.7474 - val_loss: 0.6166 - val_acc: 0.7484\n",
      "Epoch 262/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6169 - acc: 0.7471 - val_loss: 0.6172 - val_acc: 0.7472\n",
      "Epoch 263/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6162 - acc: 0.7476 - val_loss: 0.6231 - val_acc: 0.7436\n",
      "Epoch 264/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6168 - acc: 0.7472 - val_loss: 0.6158 - val_acc: 0.7470\n",
      "Epoch 265/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6161 - acc: 0.7476 - val_loss: 0.6258 - val_acc: 0.7446\n",
      "Epoch 266/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6168 - acc: 0.7473 - val_loss: 0.6191 - val_acc: 0.7460\n",
      "Epoch 267/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6164 - acc: 0.7473 - val_loss: 0.6236 - val_acc: 0.7439\n",
      "Epoch 268/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6165 - acc: 0.7473 - val_loss: 0.6186 - val_acc: 0.7469\n",
      "Epoch 269/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6164 - acc: 0.7477 - val_loss: 0.6196 - val_acc: 0.7466\n",
      "Epoch 270/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6168 - acc: 0.7474 - val_loss: 0.6178 - val_acc: 0.7469\n",
      "Epoch 271/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6162 - acc: 0.7470 - val_loss: 0.6236 - val_acc: 0.7437\n",
      "Epoch 272/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6164 - acc: 0.7473 - val_loss: 0.6223 - val_acc: 0.7437\n",
      "Epoch 273/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6165 - acc: 0.7474 - val_loss: 0.6161 - val_acc: 0.7467\n",
      "Epoch 274/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6161 - acc: 0.7481 - val_loss: 0.6181 - val_acc: 0.7473\n",
      "Epoch 275/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6162 - acc: 0.7474 - val_loss: 0.6180 - val_acc: 0.7459\n",
      "Epoch 276/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6159 - acc: 0.7475 - val_loss: 0.6178 - val_acc: 0.7474\n",
      "Epoch 277/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6164 - acc: 0.7474 - val_loss: 0.6178 - val_acc: 0.7470\n",
      "Epoch 278/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6165 - acc: 0.7475 - val_loss: 0.6179 - val_acc: 0.7468\n",
      "Epoch 279/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6159 - acc: 0.7474 - val_loss: 0.6162 - val_acc: 0.7477\n",
      "Epoch 280/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6162 - acc: 0.7475 - val_loss: 0.6157 - val_acc: 0.7479\n",
      "Epoch 281/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6160 - acc: 0.7479 - val_loss: 0.6171 - val_acc: 0.7480\n",
      "Epoch 282/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6161 - acc: 0.7473 - val_loss: 0.6160 - val_acc: 0.7481\n",
      "Epoch 283/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6159 - acc: 0.7475 - val_loss: 0.6204 - val_acc: 0.7452\n",
      "Epoch 284/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6162 - acc: 0.7476 - val_loss: 0.6138 - val_acc: 0.7486\n",
      "Epoch 285/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6161 - acc: 0.7473 - val_loss: 0.6187 - val_acc: 0.7462\n",
      "Epoch 286/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6157 - acc: 0.7477 - val_loss: 0.6180 - val_acc: 0.7462\n",
      "Epoch 287/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6159 - acc: 0.7476 - val_loss: 0.6187 - val_acc: 0.7469\n",
      "Epoch 288/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6159 - acc: 0.7476 - val_loss: 0.6173 - val_acc: 0.7465\n",
      "Epoch 289/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6156 - acc: 0.7477 - val_loss: 0.6181 - val_acc: 0.7475\n",
      "Epoch 290/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6161 - acc: 0.7473 - val_loss: 0.6168 - val_acc: 0.7471\n",
      "Epoch 291/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6154 - acc: 0.7479 - val_loss: 0.6165 - val_acc: 0.7462\n",
      "Epoch 292/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6154 - acc: 0.7475 - val_loss: 0.6176 - val_acc: 0.7471\n",
      "Epoch 293/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6156 - acc: 0.7479 - val_loss: 0.6266 - val_acc: 0.7419\n",
      "Epoch 294/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6154 - acc: 0.7477 - val_loss: 0.6160 - val_acc: 0.7480\n",
      "Epoch 295/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6156 - acc: 0.7474 - val_loss: 0.6160 - val_acc: 0.7473\n",
      "Epoch 296/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6157 - acc: 0.7476 - val_loss: 0.6166 - val_acc: 0.7484\n",
      "Epoch 297/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6156 - acc: 0.7478 - val_loss: 0.6158 - val_acc: 0.7475\n",
      "Epoch 298/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6152 - acc: 0.7476 - val_loss: 0.6138 - val_acc: 0.7481\n",
      "Epoch 299/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6153 - acc: 0.7479 - val_loss: 0.6185 - val_acc: 0.7463\n",
      "Epoch 300/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6156 - acc: 0.7476 - val_loss: 0.6128 - val_acc: 0.7501\n",
      "Epoch 301/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6152 - acc: 0.7476 - val_loss: 0.6147 - val_acc: 0.7475\n",
      "Epoch 302/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6157 - acc: 0.7478 - val_loss: 0.6218 - val_acc: 0.7457\n",
      "Epoch 303/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6151 - acc: 0.7476 - val_loss: 0.6151 - val_acc: 0.7472\n",
      "Epoch 304/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6153 - acc: 0.7474 - val_loss: 0.6197 - val_acc: 0.7444\n",
      "Epoch 305/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6150 - acc: 0.7480 - val_loss: 0.6173 - val_acc: 0.7474\n",
      "Epoch 306/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6149 - acc: 0.7480 - val_loss: 0.6144 - val_acc: 0.7481\n",
      "Epoch 307/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6153 - acc: 0.7478 - val_loss: 0.6154 - val_acc: 0.7466\n",
      "Epoch 308/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6153 - acc: 0.7475 - val_loss: 0.6150 - val_acc: 0.7473\n",
      "Epoch 309/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6154 - acc: 0.7476 - val_loss: 0.6139 - val_acc: 0.7480\n",
      "Epoch 310/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6149 - acc: 0.7478 - val_loss: 0.6156 - val_acc: 0.7478\n",
      "Epoch 311/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6152 - acc: 0.7479 - val_loss: 0.6181 - val_acc: 0.7464\n",
      "Epoch 312/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6152 - acc: 0.7473 - val_loss: 0.6133 - val_acc: 0.7478\n",
      "Epoch 313/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6148 - acc: 0.7482 - val_loss: 0.6227 - val_acc: 0.7439\n",
      "Epoch 314/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6146 - acc: 0.7479 - val_loss: 0.6235 - val_acc: 0.7421\n",
      "Epoch 315/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6150 - acc: 0.7475 - val_loss: 0.6132 - val_acc: 0.7480\n",
      "Epoch 316/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6151 - acc: 0.7479 - val_loss: 0.6175 - val_acc: 0.7463\n",
      "Epoch 317/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6151 - acc: 0.7477 - val_loss: 0.6195 - val_acc: 0.7460\n",
      "Epoch 318/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6149 - acc: 0.7481 - val_loss: 0.6138 - val_acc: 0.7486\n",
      "Epoch 319/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6146 - acc: 0.7477 - val_loss: 0.6150 - val_acc: 0.7459\n",
      "Epoch 320/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6144 - acc: 0.7481 - val_loss: 0.6151 - val_acc: 0.7462\n",
      "Epoch 321/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6148 - acc: 0.7479 - val_loss: 0.6143 - val_acc: 0.7478\n",
      "Epoch 322/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6149 - acc: 0.7482 - val_loss: 0.6141 - val_acc: 0.7486\n",
      "Epoch 323/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6144 - acc: 0.7481 - val_loss: 0.6158 - val_acc: 0.7477\n",
      "Epoch 324/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6146 - acc: 0.7477 - val_loss: 0.6138 - val_acc: 0.7485\n",
      "Epoch 325/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6146 - acc: 0.7482 - val_loss: 0.6131 - val_acc: 0.7479\n",
      "Epoch 326/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7482 - val_loss: 0.6137 - val_acc: 0.7476\n",
      "Epoch 327/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6145 - acc: 0.7483 - val_loss: 0.6145 - val_acc: 0.7477\n",
      "Epoch 328/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6148 - acc: 0.7479 - val_loss: 0.6171 - val_acc: 0.7475\n",
      "Epoch 329/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6142 - acc: 0.7476 - val_loss: 0.6158 - val_acc: 0.7472\n",
      "Epoch 330/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6142 - acc: 0.7481 - val_loss: 0.6152 - val_acc: 0.7478\n",
      "Epoch 331/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6146 - acc: 0.7479 - val_loss: 0.6143 - val_acc: 0.7479\n",
      "Epoch 332/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6147 - acc: 0.7481 - val_loss: 0.6169 - val_acc: 0.7444\n",
      "Epoch 333/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6143 - acc: 0.7482 - val_loss: 0.6201 - val_acc: 0.7463\n",
      "Epoch 334/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6147 - acc: 0.7482 - val_loss: 0.6154 - val_acc: 0.7475\n",
      "Epoch 335/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6140 - acc: 0.7482 - val_loss: 0.6131 - val_acc: 0.7487\n",
      "Epoch 336/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6144 - acc: 0.7481 - val_loss: 0.6148 - val_acc: 0.7477\n",
      "Epoch 337/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6143 - acc: 0.7481 - val_loss: 0.6148 - val_acc: 0.7495\n",
      "Epoch 338/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6141 - acc: 0.7478 - val_loss: 0.6185 - val_acc: 0.7464\n",
      "Epoch 339/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6141 - acc: 0.7480 - val_loss: 0.6176 - val_acc: 0.7470\n",
      "Epoch 340/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6141 - acc: 0.7480 - val_loss: 0.6206 - val_acc: 0.7455\n",
      "Epoch 341/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7482 - val_loss: 0.6130 - val_acc: 0.7478\n",
      "Epoch 342/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7484 - val_loss: 0.6144 - val_acc: 0.7472\n",
      "Epoch 343/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7481 - val_loss: 0.6166 - val_acc: 0.7478\n",
      "Epoch 344/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6141 - acc: 0.7481 - val_loss: 0.6135 - val_acc: 0.7485\n",
      "Epoch 345/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6137 - acc: 0.7484 - val_loss: 0.6137 - val_acc: 0.7485\n",
      "Epoch 346/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6142 - acc: 0.7480 - val_loss: 0.6151 - val_acc: 0.7480\n",
      "Epoch 347/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6141 - acc: 0.7478 - val_loss: 0.6157 - val_acc: 0.7482\n",
      "Epoch 348/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6138 - acc: 0.7482 - val_loss: 0.6173 - val_acc: 0.7460\n",
      "Epoch 349/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6133 - acc: 0.7486 - val_loss: 0.6189 - val_acc: 0.7464\n",
      "Epoch 350/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6138 - acc: 0.7482 - val_loss: 0.6159 - val_acc: 0.7475\n",
      "Epoch 351/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6140 - acc: 0.7481 - val_loss: 0.6147 - val_acc: 0.7476\n",
      "Epoch 352/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7481 - val_loss: 0.6129 - val_acc: 0.7484\n",
      "Epoch 353/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6135 - acc: 0.7481 - val_loss: 0.6129 - val_acc: 0.7488\n",
      "Epoch 354/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6136 - acc: 0.7482 - val_loss: 0.6171 - val_acc: 0.7480\n",
      "Epoch 355/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6132 - acc: 0.7481 - val_loss: 0.6176 - val_acc: 0.7474\n",
      "Epoch 356/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6141 - acc: 0.7480 - val_loss: 0.6170 - val_acc: 0.7445\n",
      "Epoch 357/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7480 - val_loss: 0.6161 - val_acc: 0.7446\n",
      "Epoch 358/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6134 - acc: 0.7481 - val_loss: 0.6162 - val_acc: 0.7460\n",
      "Epoch 359/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6138 - acc: 0.7482 - val_loss: 0.6144 - val_acc: 0.7471\n",
      "Epoch 360/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6134 - acc: 0.7484 - val_loss: 0.6131 - val_acc: 0.7479\n",
      "Epoch 361/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6133 - acc: 0.7486 - val_loss: 0.6132 - val_acc: 0.7489\n",
      "Epoch 362/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6137 - acc: 0.7483 - val_loss: 0.6127 - val_acc: 0.7487\n",
      "Epoch 363/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6139 - acc: 0.7480 - val_loss: 0.6125 - val_acc: 0.7485\n",
      "Epoch 364/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6134 - acc: 0.7485 - val_loss: 0.6143 - val_acc: 0.7479\n",
      "Epoch 365/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6128 - acc: 0.7482 - val_loss: 0.6150 - val_acc: 0.7490\n",
      "Epoch 366/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6134 - acc: 0.7482 - val_loss: 0.6120 - val_acc: 0.7502\n",
      "Epoch 367/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6136 - acc: 0.7483 - val_loss: 0.6163 - val_acc: 0.7468\n",
      "Epoch 368/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6132 - acc: 0.7483 - val_loss: 0.6148 - val_acc: 0.7472\n",
      "Epoch 369/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6137 - acc: 0.7483 - val_loss: 0.6138 - val_acc: 0.7486\n",
      "Epoch 370/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6137 - acc: 0.7483 - val_loss: 0.6162 - val_acc: 0.7465\n",
      "Epoch 371/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6134 - acc: 0.7483 - val_loss: 0.6140 - val_acc: 0.7479\n",
      "Epoch 372/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6133 - acc: 0.7486 - val_loss: 0.6121 - val_acc: 0.7489\n",
      "Epoch 373/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6132 - acc: 0.7487 - val_loss: 0.6192 - val_acc: 0.7455\n",
      "Epoch 374/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6129 - acc: 0.7484 - val_loss: 0.6220 - val_acc: 0.7439\n",
      "Epoch 375/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6135 - acc: 0.7485 - val_loss: 0.6121 - val_acc: 0.7485\n",
      "Epoch 376/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6129 - acc: 0.7489 - val_loss: 0.6150 - val_acc: 0.7472\n",
      "Epoch 377/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6131 - acc: 0.7483 - val_loss: 0.6144 - val_acc: 0.7463\n",
      "Epoch 378/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6133 - acc: 0.7486 - val_loss: 0.6114 - val_acc: 0.7501\n",
      "Epoch 379/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6132 - acc: 0.7483 - val_loss: 0.6140 - val_acc: 0.7477\n",
      "Epoch 380/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6131 - acc: 0.7487 - val_loss: 0.6217 - val_acc: 0.7443\n",
      "Epoch 381/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6127 - acc: 0.7484 - val_loss: 0.6130 - val_acc: 0.7490\n",
      "Epoch 382/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6128 - acc: 0.7484 - val_loss: 0.6134 - val_acc: 0.7490\n",
      "Epoch 383/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6131 - acc: 0.7486 - val_loss: 0.6128 - val_acc: 0.7491\n",
      "Epoch 384/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6130 - acc: 0.7483 - val_loss: 0.6119 - val_acc: 0.7492\n",
      "Epoch 385/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6131 - acc: 0.7484 - val_loss: 0.6140 - val_acc: 0.7485\n",
      "Epoch 386/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6131 - acc: 0.7483 - val_loss: 0.6124 - val_acc: 0.7484\n",
      "Epoch 387/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6130 - acc: 0.7485 - val_loss: 0.6127 - val_acc: 0.7484\n",
      "Epoch 388/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6124 - acc: 0.7487 - val_loss: 0.6184 - val_acc: 0.7462\n",
      "Epoch 389/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6128 - acc: 0.7486 - val_loss: 0.6162 - val_acc: 0.7458\n",
      "Epoch 390/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6129 - acc: 0.7482 - val_loss: 0.6132 - val_acc: 0.7494\n",
      "Epoch 391/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6131 - acc: 0.7480 - val_loss: 0.6135 - val_acc: 0.7486\n",
      "Epoch 392/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6128 - acc: 0.7486 - val_loss: 0.6154 - val_acc: 0.7464\n",
      "Epoch 393/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6127 - acc: 0.7487 - val_loss: 0.6140 - val_acc: 0.7475\n",
      "Epoch 394/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6130 - acc: 0.7481 - val_loss: 0.6146 - val_acc: 0.7478\n",
      "Epoch 395/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6130 - acc: 0.7485 - val_loss: 0.6135 - val_acc: 0.7492\n",
      "Epoch 396/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6127 - acc: 0.7484 - val_loss: 0.6165 - val_acc: 0.7467\n",
      "Epoch 397/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6125 - acc: 0.7488 - val_loss: 0.6140 - val_acc: 0.7489\n",
      "Epoch 398/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6128 - acc: 0.7487 - val_loss: 0.6143 - val_acc: 0.7484\n",
      "Epoch 399/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6127 - acc: 0.7487 - val_loss: 0.6175 - val_acc: 0.7465\n",
      "Epoch 400/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6127 - acc: 0.7485 - val_loss: 0.6128 - val_acc: 0.7490\n",
      "Epoch 401/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6125 - acc: 0.7486 - val_loss: 0.6158 - val_acc: 0.7473\n",
      "Epoch 402/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6124 - acc: 0.7486 - val_loss: 0.6171 - val_acc: 0.7456\n",
      "Epoch 403/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6133 - acc: 0.7483 - val_loss: 0.6141 - val_acc: 0.7476\n",
      "Epoch 404/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6123 - acc: 0.7486 - val_loss: 0.6173 - val_acc: 0.7444\n",
      "Epoch 405/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6120 - acc: 0.7488 - val_loss: 0.6145 - val_acc: 0.7469\n",
      "Epoch 406/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6124 - acc: 0.7486 - val_loss: 0.6132 - val_acc: 0.7475\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6123 - acc: 0.7490 - val_loss: 0.6149 - val_acc: 0.7482\n",
      "Epoch 408/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6126 - acc: 0.7489 - val_loss: 0.6127 - val_acc: 0.7468\n",
      "Epoch 409/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6122 - acc: 0.7483 - val_loss: 0.6127 - val_acc: 0.7475\n",
      "Epoch 410/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6122 - acc: 0.7487 - val_loss: 0.6170 - val_acc: 0.7453\n",
      "Epoch 411/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6126 - acc: 0.7482 - val_loss: 0.6218 - val_acc: 0.7449\n",
      "Epoch 412/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6124 - acc: 0.7483 - val_loss: 0.6146 - val_acc: 0.7474\n",
      "Epoch 413/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6123 - acc: 0.7485 - val_loss: 0.6119 - val_acc: 0.7495\n",
      "Epoch 414/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6120 - acc: 0.7489 - val_loss: 0.6218 - val_acc: 0.7448\n",
      "Epoch 415/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6124 - acc: 0.7484 - val_loss: 0.6183 - val_acc: 0.7449\n",
      "Epoch 416/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6123 - acc: 0.7485 - val_loss: 0.6139 - val_acc: 0.7483\n",
      "Epoch 417/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6118 - acc: 0.7490 - val_loss: 0.6126 - val_acc: 0.7480\n",
      "Epoch 418/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6121 - acc: 0.7488 - val_loss: 0.6115 - val_acc: 0.7494\n",
      "Epoch 419/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6122 - acc: 0.7487 - val_loss: 0.6138 - val_acc: 0.7489\n",
      "Epoch 420/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6126 - acc: 0.7481 - val_loss: 0.6141 - val_acc: 0.7483\n",
      "Epoch 421/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6122 - acc: 0.7488 - val_loss: 0.6128 - val_acc: 0.7492\n",
      "Epoch 422/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6123 - acc: 0.7487 - val_loss: 0.6124 - val_acc: 0.7485\n",
      "Epoch 423/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6120 - acc: 0.7486 - val_loss: 0.6157 - val_acc: 0.7464\n",
      "Epoch 424/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6123 - acc: 0.7487 - val_loss: 0.6101 - val_acc: 0.7489\n",
      "Epoch 425/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6124 - acc: 0.7486 - val_loss: 0.6129 - val_acc: 0.7483\n",
      "Epoch 426/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6120 - acc: 0.7486 - val_loss: 0.6151 - val_acc: 0.7473\n",
      "Epoch 427/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6121 - acc: 0.7485 - val_loss: 0.6120 - val_acc: 0.7477\n",
      "Epoch 428/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6120 - acc: 0.7489 - val_loss: 0.6155 - val_acc: 0.7485\n",
      "Epoch 429/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6121 - acc: 0.7487 - val_loss: 0.6172 - val_acc: 0.7471\n",
      "Epoch 430/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6121 - acc: 0.7484 - val_loss: 0.6140 - val_acc: 0.7483\n",
      "Epoch 431/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6121 - acc: 0.7486 - val_loss: 0.6144 - val_acc: 0.7474\n",
      "Epoch 432/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6119 - acc: 0.7488 - val_loss: 0.6114 - val_acc: 0.7488\n",
      "Epoch 433/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6121 - acc: 0.7488 - val_loss: 0.6127 - val_acc: 0.7487\n",
      "Epoch 434/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6119 - acc: 0.7486 - val_loss: 0.6192 - val_acc: 0.7462\n",
      "Epoch 435/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6115 - acc: 0.7488 - val_loss: 0.6147 - val_acc: 0.7485\n",
      "Epoch 436/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6119 - acc: 0.7488 - val_loss: 0.6129 - val_acc: 0.7495\n",
      "Epoch 437/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6119 - acc: 0.7485 - val_loss: 0.6133 - val_acc: 0.7496\n",
      "Epoch 438/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6118 - acc: 0.7487 - val_loss: 0.6140 - val_acc: 0.7477\n",
      "Epoch 439/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6119 - acc: 0.7485 - val_loss: 0.6091 - val_acc: 0.7502\n",
      "Epoch 440/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6118 - acc: 0.7490 - val_loss: 0.6141 - val_acc: 0.7496\n",
      "Epoch 441/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7489 - val_loss: 0.6126 - val_acc: 0.7491\n",
      "Epoch 442/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6117 - acc: 0.7485 - val_loss: 0.6142 - val_acc: 0.7470\n",
      "Epoch 443/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6116 - acc: 0.7488 - val_loss: 0.6121 - val_acc: 0.7497\n",
      "Epoch 444/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7488 - val_loss: 0.6128 - val_acc: 0.7489\n",
      "Epoch 445/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6118 - acc: 0.7488 - val_loss: 0.6107 - val_acc: 0.7487\n",
      "Epoch 446/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6118 - acc: 0.7487 - val_loss: 0.6145 - val_acc: 0.7489\n",
      "Epoch 447/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6116 - acc: 0.7489 - val_loss: 0.6124 - val_acc: 0.7501\n",
      "Epoch 448/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6117 - acc: 0.7490 - val_loss: 0.6135 - val_acc: 0.7482\n",
      "Epoch 449/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7489 - val_loss: 0.6155 - val_acc: 0.7482\n",
      "Epoch 450/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6116 - acc: 0.7487 - val_loss: 0.6112 - val_acc: 0.7497\n",
      "Epoch 451/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6115 - acc: 0.7491 - val_loss: 0.6121 - val_acc: 0.7496\n",
      "Epoch 452/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7491 - val_loss: 0.6138 - val_acc: 0.7485\n",
      "Epoch 453/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6118 - acc: 0.7487 - val_loss: 0.6142 - val_acc: 0.7484\n",
      "Epoch 454/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6115 - acc: 0.7488 - val_loss: 0.6124 - val_acc: 0.7498\n",
      "Epoch 455/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7485 - val_loss: 0.6132 - val_acc: 0.7484\n",
      "Epoch 456/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7489 - val_loss: 0.6127 - val_acc: 0.7489\n",
      "Epoch 457/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7489 - val_loss: 0.6163 - val_acc: 0.7459\n",
      "Epoch 458/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7491 - val_loss: 0.6107 - val_acc: 0.7497\n",
      "Epoch 459/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7487 - val_loss: 0.6111 - val_acc: 0.7483\n",
      "Epoch 460/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7491 - val_loss: 0.6105 - val_acc: 0.7500\n",
      "Epoch 461/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6115 - acc: 0.7490 - val_loss: 0.6187 - val_acc: 0.7470\n",
      "Epoch 462/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7492 - val_loss: 0.6108 - val_acc: 0.7497\n",
      "Epoch 463/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6116 - acc: 0.7490 - val_loss: 0.6139 - val_acc: 0.7459\n",
      "Epoch 464/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6119 - acc: 0.7487 - val_loss: 0.6128 - val_acc: 0.7481\n",
      "Epoch 465/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7490 - val_loss: 0.6130 - val_acc: 0.7486\n",
      "Epoch 466/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7489 - val_loss: 0.6141 - val_acc: 0.7478\n",
      "Epoch 467/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7490 - val_loss: 0.6130 - val_acc: 0.7478\n",
      "Epoch 468/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7485 - val_loss: 0.6119 - val_acc: 0.7485\n",
      "Epoch 469/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7489 - val_loss: 0.6099 - val_acc: 0.7500\n",
      "Epoch 470/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7493 - val_loss: 0.6133 - val_acc: 0.7483\n",
      "Epoch 471/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6115 - acc: 0.7489 - val_loss: 0.6131 - val_acc: 0.7476\n",
      "Epoch 472/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7489 - val_loss: 0.6109 - val_acc: 0.7493\n",
      "Epoch 473/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6115 - acc: 0.7491 - val_loss: 0.6126 - val_acc: 0.7480\n",
      "Epoch 474/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6112 - acc: 0.7489 - val_loss: 0.6106 - val_acc: 0.7485\n",
      "Epoch 475/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7491 - val_loss: 0.6151 - val_acc: 0.7476\n",
      "Epoch 476/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7488 - val_loss: 0.6106 - val_acc: 0.7485\n",
      "Epoch 477/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6110 - acc: 0.7493 - val_loss: 0.6155 - val_acc: 0.7473\n",
      "Epoch 478/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6109 - acc: 0.7494 - val_loss: 0.6123 - val_acc: 0.7489\n",
      "Epoch 479/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6110 - acc: 0.7494 - val_loss: 0.6113 - val_acc: 0.7496\n",
      "Epoch 480/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6108 - acc: 0.7491 - val_loss: 0.6124 - val_acc: 0.7484\n",
      "Epoch 481/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7491 - val_loss: 0.6138 - val_acc: 0.7459\n",
      "Epoch 482/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6109 - acc: 0.7491 - val_loss: 0.6132 - val_acc: 0.7483\n",
      "Epoch 483/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7488 - val_loss: 0.6094 - val_acc: 0.7509\n",
      "Epoch 484/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6108 - acc: 0.7494 - val_loss: 0.6117 - val_acc: 0.7488\n",
      "Epoch 485/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7489 - val_loss: 0.6120 - val_acc: 0.7490\n",
      "Epoch 486/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6107 - acc: 0.7490 - val_loss: 0.6126 - val_acc: 0.7480\n",
      "Epoch 487/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7492 - val_loss: 0.6155 - val_acc: 0.7466\n",
      "Epoch 488/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6106 - acc: 0.7494 - val_loss: 0.6189 - val_acc: 0.7448\n",
      "Epoch 489/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6113 - acc: 0.7488 - val_loss: 0.6102 - val_acc: 0.7497\n",
      "Epoch 490/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6114 - acc: 0.7488 - val_loss: 0.6162 - val_acc: 0.7473\n",
      "Epoch 491/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6106 - acc: 0.7492 - val_loss: 0.6132 - val_acc: 0.7478\n",
      "Epoch 492/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7490 - val_loss: 0.6177 - val_acc: 0.7466\n",
      "Epoch 493/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6110 - acc: 0.7490 - val_loss: 0.6131 - val_acc: 0.7490\n",
      "Epoch 494/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6110 - acc: 0.7492 - val_loss: 0.6113 - val_acc: 0.7484\n",
      "Epoch 495/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6105 - acc: 0.7492 - val_loss: 0.6119 - val_acc: 0.7482\n",
      "Epoch 496/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6107 - acc: 0.7494 - val_loss: 0.6133 - val_acc: 0.7483\n",
      "Epoch 497/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6108 - acc: 0.7491 - val_loss: 0.6115 - val_acc: 0.7479\n",
      "Epoch 498/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7490 - val_loss: 0.6147 - val_acc: 0.7472\n",
      "Epoch 499/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6106 - acc: 0.7491 - val_loss: 0.6136 - val_acc: 0.7483\n",
      "Epoch 500/500\n",
      "768000/768000 [==============================] - 3s 4us/step - loss: 0.6111 - acc: 0.7490 - val_loss: 0.6131 - val_acc: 0.7478\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train_scaled,y_train_scaled,verbose=True,epochs = 500,batch_size=768,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2c517550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'],label = 'loss')\n",
    "plt.plot(hist.history['val_loss'],label = 'val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a2c4cfb70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['acc'],label = 'acc')\n",
    "plt.plot(hist.history['val_acc'],label = 'val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prediction)):\n",
    "    idx = np.argmax(prediction[i])\n",
    "    prediction[i] = 0\n",
    "    prediction[i][idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(prediction,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_scaled = np.argmax(y_test_scaled,axis = 1)\n",
    "y_test_scaled = y_test_scaled.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7474791666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(prediction == y_test_scaled)/len(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So accuracy on test set is 74.74%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
